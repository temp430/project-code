{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "110NRuDbCZ-f",
        "outputId": "73251e3a-db3c-4364-ba8b-a196d5c78323"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\yuany\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from gensim.corpora import Dictionary\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from gensim.test.utils import common_corpus, common_dictionary\n",
        "import gensim\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim.test.utils import datapath\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "with open('chunk_8.txt', 'r', encoding='latin-1') as f:\n",
        "    clean_cont = f.read().splitlines()\n",
        "\n",
        "doc=[i.replace('\\xe2\\x80\\x9c','') for i in clean_cont ]\n",
        "doc=[i.replace('\\xe2\\x80\\x9d','') for i in doc ]\n",
        "doc=[i.replace('\\xe2\\x80\\x99s','') for i in doc ]\n",
        "docs = [x for x in doc if x != ' ']\n",
        "docss = [x for x in docs if x != '']\n",
        "financedoc=[re.sub(\"[^a-zA-Z]+\", \" \", s) for s in docss]\n",
        "\n",
        "# Tokenize the preprocessed text documents\n",
        "tokenized_docs = [doc.split() for doc in financedoc]\n",
        "# Define English stopwords\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "# Define additional words to include as stopwords\n",
        "additional_stopwords = {\"A\", \"B\", \"C\",\"S\" ,\"a\", \"b\", \"c\", \"s\",\"Total\",\"Period\",\"Other\",\"ended\", \"Whether\", \"wells\", \"Fargo\", \"Wells\"}\n",
        "# Add the additional stopwords to the set\n",
        "english_stopwords.update(additional_stopwords)\n",
        "\n",
        "# Step 1: Remove English stopwords during tokenization\n",
        "tokenized_docs_no_stopwords = [[word for word in doc if word not in english_stopwords] for doc in tokenized_docs]\n",
        "\n",
        "# Step 2: Create a Gensim dictionary\n",
        "gensim_dictionary = Dictionary(tokenized_docs_no_stopwords)\n",
        "\n",
        "# Optionally: Filter out very rare and very common words\n",
        "gensim_dictionary.filter_extremes(no_below=5, no_above=0.5)  # Adjust as needed\n",
        "\n",
        "# Step 3: Convert tokenized documents into the Gensim corpus format\n",
        "corpus = [gensim_dictionary.doc2bow(doc) for doc in tokenized_docs_no_stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES_mgTENCi9R",
        "outputId": "3cc30737-1207-4e7c-a8a6-d4f73cf8e6ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.056*\"financial\" + 0.044*\"liabilities\" + 0.036*\"billion\" + 0.029*\"income\" + 0.029*\"September\" + 0.025*\"assets\" + 0.025*\"consolidated\" + 0.025*\"entities\" + 0.022*\"We\" + 0.022*\"loans\"'),\n",
              " (1,\n",
              "  '0.058*\"securities\" + 0.032*\"Securities\" + 0.029*\"loans\" + 0.026*\"U\" + 0.023*\"stock\" + 0.020*\"equity\" + 0.020*\"fair\" + 0.019*\"millions\" + 0.017*\"value\" + 0.016*\"federal\"'),\n",
              " (2,\n",
              "  '0.055*\"million\" + 0.036*\"Net\" + 0.035*\"accounting\" + 0.032*\"value\" + 0.032*\"change\" + 0.031*\"securities\" + 0.031*\"statements\" + 0.025*\"fair\" + 0.024*\"The\" + 0.021*\"interests\"'),\n",
              " (3,\n",
              "  '0.057*\"stock\" + 0.048*\"securities\" + 0.035*\"term\" + 0.029*\"net\" + 0.029*\"assets\" + 0.026*\"Common\" + 0.023*\"investments\" + 0.023*\"We\" + 0.023*\"debt\" + 0.023*\"shares\"'),\n",
              " (4,\n",
              "  '0.052*\"The\" + 0.038*\"Net\" + 0.032*\"billion\" + 0.029*\"cash\" + 0.027*\"activities\" + 0.026*\"net\" + 0.026*\"losses\" + 0.026*\"ASU\" + 0.023*\"assets\" + 0.023*\"stock\"')]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lda = LdaMulticore(corpus, id2word=gensim_dictionary, num_topics=5)\n",
        "benchmark_model = datapath(\"benchmark_model\")\n",
        "lda.save(benchmark_model)\n",
        "lda.print_topics(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subsidiaries\n"
          ]
        }
      ],
      "source": [
        "print(gensim_dictionary[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Load the Mapping of Lower-Cased Vocabulary Items to Their Most Common Surface Form__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Surface_Form': 'Modeled_Term', 'income statement': 'financial_statements', 'statement of earnings': 'financial_statements', 'profit and loss statement': 'financial_statements', 'balance sheet              ': 'financial_statements', 'income statement           ': 'financial_statements', 'P&L statement              ': 'financial_statements', 'Comprehensive Income': 'financial_statements', 'Changes in Equity': 'financial_statements', 'Consolidated Statement': 'financial_statements', 'securities': 'shares'}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy\n",
        "# Load surface form mappings here\n",
        "\n",
        "current_directory = os.getcwd()\n",
        "filename=\"Vocab2SurfaceFormMapping.tsv\"\n",
        "fp = open(os.path.join(current_directory, filename))    \n",
        "\n",
        "vocabToSurfaceFormHash = {}\n",
        "\n",
        "# Each line in the file has two tab separated fields;\n",
        "# the first is the vocabulary item used during modeling\n",
        "# and the second is its most common surface form in the \n",
        "# original data\n",
        "for stringIn in fp.readlines():\n",
        "    fields = stringIn.strip().split(\"\\t\")\n",
        "    if len(fields) != 2:\n",
        "        print (\"Warning: Bad line in surface form mapping file: %s\" % stringIn)\n",
        "    elif fields[0] == \"\" or fields[1] == \"\":\n",
        "        print (\"Warning: Bad line in surface form mapping file: %s\" % stringIn)\n",
        "    else:\n",
        "        vocabToSurfaceFormHash[fields[0]] = fields[1]\n",
        "fp.close()\n",
        "print(vocabToSurfaceFormHash)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dictionary<140 unique tokens: ['Company', 'Subsidiaries', 'Consolidated', 'Statement', 'millions']...>\n",
            "Term index: 1\n",
            "Surface form: Subsidiaries\n",
            "Modeled form: Subsidiaries\n"
          ]
        }
      ],
      "source": [
        "def CreateTermIDToSurfaceFormMapping(id2token, token2surfaceform):\n",
        "    termIDToSurfaceFormMap = []\n",
        "    for i in range(0, len(id2token)):\n",
        "        if id2token[i] in token2surfaceform:\n",
        "            termIDToSurfaceFormMap.append(token2surfaceform[id2token[i]])\n",
        "        else:\n",
        "             termIDToSurfaceFormMap.append(id2token[i])\n",
        "\n",
        "    return termIDToSurfaceFormMap;\n",
        "\n",
        "termIDToSurfaceFormMap = CreateTermIDToSurfaceFormMapping(gensim_dictionary, vocabToSurfaceFormHash);\n",
        "\n",
        "\n",
        "print(gensim_dictionary)\n",
        "# print out the modeled token form and the best matching surface for the token with the index value of 18\n",
        "i = 1\n",
        "print('Term index:', i)\n",
        "print('Surface form:', gensim_dictionary[i])\n",
        "print('Modeled form:', termIDToSurfaceFormMap[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1299, 5)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# To retrieve all topics and their probabilities we must set the LDA minimum probability setting to zero\n",
        "lda.minimum_probability = 0\n",
        "\n",
        "# This function generates the topic probabilities for each doc from the trained LDA model\n",
        "# The probabilities are placed in a single matrix where the rows are documents and columns are topics\n",
        "def ExtractDocTopicProbsMatrix(corpus,lda):\n",
        "    # Initialize the matrix\n",
        "    docTopicProbs = numpy.zeros((len(corpus),lda.num_topics))\n",
        "    for docID in range(0,len(corpus)):\n",
        "        for topicProb in lda[corpus[docID]]:\n",
        "            docTopicProbs[docID,topicProb[0]]=topicProb[1]\n",
        "    return docTopicProbs    \n",
        "\n",
        "# docTopicProbs[docID,TopicID] --> P(topic|doc)\n",
        "docTopicProbs = ExtractDocTopicProbsMatrix(corpus, lda)\n",
        "\n",
        "docTopicProbs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Compute the Global Topic Likelihood Scores P(topic)__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.20540022 0.19566252 0.19137388 0.20977199 0.19779139]\n"
          ]
        }
      ],
      "source": [
        "# Computing the global topic likelihoods by aggregating topic probabilities over all documents\n",
        "# topicProbs[topicID] --> P(topic)\n",
        "def ComputeTopicProbs(docTopicProbs):\n",
        "    topicProbs = docTopicProbs.sum(axis=0) \n",
        "    topicProbs = topicProbs/sum(topicProbs)\n",
        "    return topicProbs\n",
        "\n",
        "topicProbs = ComputeTopicProbs(docTopicProbs)\n",
        "print(topicProbs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Convert the Topic Language Model Information P(term|topic) from the LDA Model into a NumPy Representation__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.01459535 0.00074783 0.00075402 0.00416383 0.00081808 0.02861062\n",
            "  0.00430825 0.01811735 0.00769313 0.00770886 0.01261785 0.00085736\n",
            "  0.00422088 0.00078074 0.01465043 0.00075649 0.00077692 0.00078048\n",
            "  0.00424528 0.00768563 0.0007275  0.01109676 0.00770423 0.00421305\n",
            "  0.00772286 0.00429639 0.00074515 0.01815366 0.00076806 0.0008281\n",
            "  0.00426105 0.01575166 0.00884713 0.00905604 0.00193495 0.01937461\n",
            "  0.00170668 0.00074355 0.00523751 0.01567702 0.00072523 0.00183846\n",
            "  0.00429843 0.02530282 0.0112521  0.00079374 0.01125694 0.00769996\n",
            "  0.00774281 0.00072087 0.00075013 0.00073867 0.00768327 0.0007645\n",
            "  0.02161301 0.00076969 0.00414628 0.04373571 0.01123456 0.00442719\n",
            "  0.00080819 0.00102779 0.00079754 0.00075343 0.01458634 0.01117739\n",
            "  0.00075263 0.00076337 0.00423491 0.01463752 0.00772596 0.02856942\n",
            "  0.0076986  0.02512334 0.02504873 0.02149512 0.00423174 0.0133269\n",
            "  0.03556632 0.00414146 0.00074027 0.00073912 0.00075677 0.00074083\n",
            "  0.00430974 0.00779639 0.00432736 0.0042165  0.00423258 0.00420535\n",
            "  0.00757539 0.00072969 0.00418248 0.00075257 0.00074327 0.02131493\n",
            "  0.01819202 0.01452834 0.05645582 0.02163814 0.00076147 0.00420801\n",
            "  0.0007323  0.00766234 0.00074483 0.00417762 0.01114585 0.00077316\n",
            "  0.01119487 0.00771937 0.00769318 0.00079015 0.00759688 0.0042314\n",
            "  0.00416777 0.01460392 0.004264   0.01803668 0.00424033 0.00423641\n",
            "  0.00073788 0.00076453 0.00075656 0.00760394 0.00072149 0.00422096\n",
            "  0.00421271 0.0007227  0.00073818 0.00075443 0.00767636 0.00077321\n",
            "  0.00075403 0.00074424 0.00075067 0.0007659  0.00073363 0.00074613\n",
            "  0.00424194 0.00072609]\n",
            " [0.01326649 0.01322857 0.01011677 0.00699872 0.01936581 0.00712571\n",
            "  0.00387535 0.0007217  0.00066392 0.00701412 0.05750194 0.00705979\n",
            "  0.00068955 0.00385592 0.00072235 0.00070262 0.00386991 0.01021477\n",
            "  0.00071744 0.01015512 0.00066698 0.0006911  0.00704624 0.00697429\n",
            "  0.00070068 0.00712007 0.00067308 0.00708491 0.00383852 0.01645861\n",
            "  0.0007092  0.01019673 0.00065812 0.00065565 0.00698876 0.00069709\n",
            "  0.00382499 0.00068166 0.0006557  0.00067554 0.00065692 0.00382477\n",
            "  0.0007408  0.00717939 0.01969841 0.0102186  0.01655846 0.0038129\n",
            "  0.00701919 0.00381909 0.01334924 0.00385318 0.00068407 0.01640808\n",
            "  0.02911622 0.00070017 0.00068793 0.00390789 0.01020414 0.01978054\n",
            "  0.01017482 0.02292071 0.00385877 0.00067736 0.00068772 0.01326449\n",
            "  0.01333086 0.00381818 0.00384264 0.01643368 0.00067012 0.00077895\n",
            "  0.00066303 0.00386612 0.00385145 0.00384197 0.00068695 0.00071027\n",
            "  0.00702406 0.00384541 0.00067596 0.0101477  0.00702142 0.00700381\n",
            "  0.00391479 0.00390596 0.01030036 0.00700299 0.01011407 0.00383417\n",
            "  0.00388536 0.00384506 0.00687078 0.00385221 0.01322848 0.00075161\n",
            "  0.01629529 0.0006821  0.00713898 0.01336753 0.00381431 0.00069795\n",
            "  0.01331692 0.00066729 0.02597745 0.01015568 0.00382945 0.00070491\n",
            "  0.00387639 0.003854   0.00377575 0.00071368 0.00382234 0.00384164\n",
            "  0.00698374 0.00386107 0.0164603  0.00706583 0.00386414 0.00387528\n",
            "  0.01329131 0.0070245  0.00697899 0.01012394 0.00065863 0.00384362\n",
            "  0.01016516 0.01331964 0.00384876 0.01334035 0.00701472 0.00998208\n",
            "  0.01017899 0.00701357 0.00067745 0.03224364 0.01648437 0.01647806\n",
            "  0.010172   0.01013145]\n",
            " [0.00080684 0.00074769 0.00419306 0.00077653 0.00084088 0.01131064\n",
            "  0.00785697 0.01113999 0.00422486 0.00771823 0.03143127 0.03557425\n",
            "  0.00074907 0.00078734 0.00415523 0.00076845 0.00259871 0.00076997\n",
            "  0.00418237 0.00073998 0.0076994  0.01467311 0.02147257 0.0111717\n",
            "  0.00761289 0.02357715 0.0130592  0.03057735 0.01112838 0.00081641\n",
            "  0.00428657 0.00673883 0.00306807 0.00285003 0.00309127 0.00294969\n",
            "  0.01020258 0.00772433 0.00319272 0.00315457 0.00423401 0.00657441\n",
            "  0.00429676 0.00794928 0.02516177 0.01817339 0.03210596 0.00771407\n",
            "  0.0042334  0.01120495 0.00773912 0.00772209 0.0007672  0.00771393\n",
            "  0.00433638 0.00076858 0.00421143 0.00292955 0.01126627 0.01464051\n",
            "  0.00079955 0.00786967 0.00776978 0.00420596 0.00076831 0.00077408\n",
            "  0.00074823 0.00415952 0.00769613 0.01125511 0.00073443 0.01478505\n",
            "  0.0146548  0.00424229 0.00423006 0.00078635 0.00759636 0.05451726\n",
            "  0.00781926 0.00775185 0.01117071 0.00420939 0.01811175 0.00762337\n",
            "  0.03205672 0.03545112 0.00439264 0.01471051 0.00073406 0.0042053\n",
            "  0.00078548 0.00073161 0.0042035  0.00075505 0.00424856 0.00276978\n",
            "  0.00993389 0.00256738 0.01813787 0.00784363 0.00075286 0.00423099\n",
            "  0.00073073 0.00073657 0.00075526 0.00421477 0.0042124  0.0076366\n",
            "  0.0042669  0.00415236 0.00080927 0.00425597 0.00076269 0.01106851\n",
            "  0.00769864 0.00424722 0.0042605  0.00079475 0.00077251 0.0007771\n",
            "  0.00420301 0.00076485 0.00422312 0.00076939 0.00072202 0.00422636\n",
            "  0.00419908 0.00420519 0.00073994 0.00075813 0.01119361 0.00078311\n",
            "  0.01810094 0.01106274 0.00076423 0.00077914 0.00072241 0.00074936\n",
            "  0.00421241 0.00771576]\n",
            " [0.0007254  0.00068053 0.00381053 0.00698304 0.00382526 0.01977365\n",
            "  0.0290067  0.01337141 0.0164807  0.01649725 0.04809734 0.00077692\n",
            "  0.00068505 0.00069902 0.00385674 0.00682345 0.00072238 0.00070621\n",
            "  0.01002106 0.00380698 0.01011488 0.00387586 0.0007416  0.00067366\n",
            "  0.00382461 0.00075969 0.00067871 0.00075767 0.00694096 0.02272156\n",
            "  0.01309556 0.00707797 0.00380176 0.00381156 0.00705659 0.01017101\n",
            "  0.00700667 0.02276871 0.01331956 0.01017735 0.01963303 0.0069895\n",
            "  0.03525364 0.02876564 0.01019625 0.01958947 0.01336947 0.00065398\n",
            "  0.01641339 0.0069828  0.01328075 0.01007666 0.01322052 0.0006899\n",
            "  0.00071755 0.00068704 0.00070886 0.00072104 0.02273738 0.01934474\n",
            "  0.01960961 0.05742995 0.02583448 0.01963969 0.00695732 0.00068525\n",
            "  0.0069133  0.01329859 0.0006842  0.00072171 0.01006112 0.00393605\n",
            "  0.00066296 0.000697   0.00068968 0.00072757 0.00068079 0.00385702\n",
            "  0.00074264 0.00376358 0.00695606 0.00066275 0.0038368  0.00384515\n",
            "  0.0039095  0.00387453 0.01963359 0.01013545 0.00067206 0.00067963\n",
            "  0.01016986 0.01329128 0.00073441 0.00068247 0.00066896 0.00704761\n",
            "  0.0007724  0.00068483 0.00079432 0.02276044 0.00384566 0.0069616\n",
            "  0.00379439 0.00067681 0.0038269  0.00066635 0.00065766 0.00689222\n",
            "  0.01326198 0.00069794 0.00686476 0.00070225 0.00069941 0.00068586\n",
            "  0.00067513 0.00699061 0.00386775 0.00697514 0.01010893 0.01320311\n",
            "  0.00066674 0.01320825 0.00069207 0.00069422 0.00065418 0.00068644\n",
            "  0.00065084 0.00065661 0.0100904  0.00069245 0.00065943 0.00384643\n",
            "  0.00068504 0.00068349 0.00069078 0.00070365 0.00066232 0.00067215\n",
            "  0.0038338  0.00065452]\n",
            " [0.00382063 0.00380395 0.00067876 0.00692875 0.0007705  0.02256261\n",
            "  0.02583384 0.00070297 0.00066191 0.00381713 0.01974879 0.03831533\n",
            "  0.02254291 0.02560865 0.01322613 0.01323904 0.02709985 0.02878217\n",
            "  0.00384674 0.00066431 0.00065496 0.00382867 0.0007388  0.00066649\n",
            "  0.00379331 0.05226818 0.00516915 0.00516834 0.00069603 0.0038466\n",
            "  0.00069947 0.01941195 0.00377902 0.00377977 0.01940203 0.00067214\n",
            "  0.00378499 0.0006714  0.0037881  0.00067708 0.00065241 0.00379197\n",
            "  0.00073034 0.02274621 0.00071077 0.00070997 0.0007327  0.00378567\n",
            "  0.00069143 0.00064763 0.00067199 0.00066154 0.00068483 0.00067654\n",
            "  0.00387642 0.01615958 0.01317006 0.0007249  0.00072363 0.00085239\n",
            "  0.01318411 0.0227342  0.00070386 0.00693111 0.00067759 0.00381448\n",
            "  0.00067231 0.00068226 0.00370724 0.00071838 0.00066871 0.01938857\n",
            "  0.00065696 0.01320388 0.00069245 0.00380419 0.00692655 0.00699448\n",
            "  0.03210251 0.00383866 0.00379841 0.0069202  0.00381443 0.00380701\n",
            "  0.01630849 0.0007053  0.02555681 0.00068602 0.00692479 0.00688449\n",
            "  0.00377357 0.00377451 0.00384663 0.01622469 0.00067289 0.00212999\n",
            "  0.00509993 0.00218321 0.0101729  0.00389679 0.0194263  0.00373589\n",
            "  0.00065917 0.01318217 0.00067358 0.00065721 0.00065981 0.00382721\n",
            "  0.01324851 0.013191   0.00072522 0.01297793 0.00691939 0.00067835\n",
            "  0.00066533 0.00381934 0.01634724 0.00382979 0.01947889 0.00378864\n",
            "  0.0006587  0.00380141 0.00684651 0.00067492 0.0163186  0.01944689\n",
            "  0.00064572 0.00065244 0.00377592 0.0099719  0.00065763 0.00382427\n",
            "  0.00382557 0.00068403 0.0193523  0.00383269 0.00694072 0.00374878\n",
            "  0.01636247 0.00065156]]\n"
          ]
        }
      ],
      "source": [
        "def ExtractTopicLMMatrix(lda):\n",
        "    # Initialize the matrix\n",
        "    docTopicProbs = numpy.zeros((lda.num_topics,lda.num_terms))\n",
        "    for topicID in range(0,lda.num_topics):\n",
        "        termProbsList = lda.get_topic_terms(topicID,lda.num_terms)\n",
        "        for termProb in termProbsList:\n",
        "            docTopicProbs[topicID,termProb[0]]=termProb[1]\n",
        "    return docTopicProbs\n",
        "    \n",
        "# topicTermProbs[topicID,termID] --> P(term|topic)\n",
        "topicTermProbs = ExtractTopicLMMatrix(lda)\n",
        "print(topicTermProbs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Compute P(topic,term), P(term), and P(topic|term)__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the joint likelihoods of topics and terms\n",
        "# jointTopicTermProbs[topicID,termID] --> P(topic,term) = P(term|topic)*P(topic)\n",
        "jointTopicTermProbs = numpy.diag(topicProbs).dot(topicTermProbs) \n",
        "\n",
        "# termProbs[termID] --> P(term)\n",
        "termProbs = jointTopicTermProbs.sum(axis=0)\n",
        "\n",
        "# topicProbsPermTerm[topicID,termID] --> P(topic|term)\n",
        "topicProbsPerTerm = jointTopicTermProbs / termProbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1: securities --> 0.033853\n",
            "2: stock --> 0.022746\n",
            "3: assets --> 0.018656\n",
            "4: financial --> 0.018643\n",
            "5: income --> 0.018046\n",
            "6: The --> 0.017285\n",
            "7: billion --> 0.016681\n",
            "8: Net --> 0.016107\n",
            "9: million --> 0.015502\n",
            "10: value --> 0.014646\n",
            "11: net --> 0.014341\n",
            "12: We --> 0.014106\n",
            "13: September --> 0.013511\n",
            "14: fair --> 0.013260\n",
            "15: ASU --> 0.012918\n",
            "16: statements --> 0.012148\n",
            "17: loans --> 0.011883\n",
            "18: Cash --> 0.011844\n",
            "19: change --> 0.011832\n",
            "20: equity --> 0.011808\n",
            "21: debt --> 0.011373\n",
            "22: liabilities --> 0.010603\n",
            "23: accounting --> 0.010102\n",
            "24: Note --> 0.009997\n",
            "25: sale --> 0.009890\n"
          ]
        }
      ],
      "source": [
        "# Print most frequent words in LDA vocab\n",
        "mostFrequentTermIDs = (-termProbs).argsort()\n",
        "for i in range(0,25):\n",
        "    print (\"%d: %s --> %f\" % (i+1, gensim_dictionary[mostFrequentTermIDs[i]], termProbs[mostFrequentTermIDs[i]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Compute WPMI__  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5, 140)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "topicTermWPMI =(jointTopicTermProbs.T * numpy.log(topicProbsPerTerm.T / topicProbs)).T\n",
        "topicTermWPMI.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Compute Topic to Document Purity measure for Each Topic__  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "topicPurity = numpy.exp(((docTopicProbs * numpy.log(docTopicProbs)).sum(axis=0))/(docTopicProbs).sum(axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Create Topic Summaries__  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                        WPMI                                                 Prob\n",
            " 1:                             million ---> 0.013120                                million ---> 0.054517\n",
            " 2:                          accounting ---> 0.008517                                    Net ---> 0.035574\n",
            " 3:                              change ---> 0.006115                             accounting ---> 0.035451\n",
            " 4:                          statements ---> 0.005402                                  value ---> 0.032106\n",
            " 5:                                 Net ---> 0.005394                                 change ---> 0.032057\n",
            " 6:                               value ---> 0.004823                                 shares ---> 0.031431\n",
            " 7:                           interests ---> 0.004393                             statements ---> 0.030577\n",
            " 8:                           available ---> 0.003542                                   fair ---> 0.025162\n",
            " 9:                        compensation ---> 0.003523                                    The ---> 0.023577\n",
            "10:                             certain ---> 0.003139                              interests ---> 0.021473\n",
            "11:                                fair ---> 0.003084                                   sale ---> 0.018173\n",
            "12:                                part ---> 0.002989                              financial ---> 0.018138\n",
            "13:                               Table ---> 0.002196                           compensation ---> 0.018112\n",
            "14:                             related ---> 0.002163                              available ---> 0.018101\n",
            "15:                                Fair ---> 0.002147                              September ---> 0.014785\n"
          ]
        }
      ],
      "source": [
        "topicID = 2\n",
        "\n",
        "highestWPMITermIDs = (-topicTermWPMI[topicID]).argsort()\n",
        "highestProbTermIDs = (-topicTermProbs[topicID]).argsort()\n",
        "print (\"                                        WPMI                                                 Prob\")\n",
        "for i in range(0,15):\n",
        "    print (\"%2d: %35s ---> %8.6f    %35s ---> %8.6f\" % (i+1, \n",
        "                                                        termIDToSurfaceFormMap[highestWPMITermIDs[i]], \n",
        "                                                        topicTermWPMI[topicID,highestWPMITermIDs[i]],\n",
        "                                                        termIDToSurfaceFormMap[highestProbTermIDs[i]], \n",
        "                                                        topicTermProbs[topicID,highestProbTermIDs[i]]))       "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def CreateTopicSummaries(topicTermScores, id2token, tokenid2surfaceform, maxStringLen):\n",
        "    reIgnore = re.compile('^[a-z]\\.$')\n",
        "    reAcronym = re.compile('^[A-Z]+$')\n",
        "    topicSummaries = []\n",
        "    for topicID in range(0,len(topicTermScores)):\n",
        "        rankedTermIDs = (-topicTermScores[topicID]).argsort()\n",
        "        maxNumTerms = len(rankedTermIDs)\n",
        "        termIndex = 0\n",
        "        stop = 0\n",
        "        outputTokens = []\n",
        "        prevAcronyms = []\n",
        "        topicSummary = \"\"\n",
        "        while not stop:\n",
        "            # If we've run out of tokens then stop...\n",
        "            if (termIndex>=maxNumTerms):\n",
        "                stop=1\n",
        "            # ...otherwise consider adding next token to summary\n",
        "            else:\n",
        "                nextToken = id2token[rankedTermIDs[termIndex]]\n",
        "                nextTokenOut = tokenid2surfaceform[rankedTermIDs[termIndex]]\n",
        "                keepToken = 1\n",
        "                \n",
        "                # Prepare to test current word as an acronym or a string that reduces to an acronym\n",
        "                nextTokenIsAcronym = 0\n",
        "                nextTokenAbbrev = \"\"\n",
        "                if reAcronym.match(nextTokenOut) != None:\n",
        "                    nextTokenIsAcronym = 1\n",
        "                else:\n",
        "                    subTokens = nextToken.split('_')\n",
        "                    if (len(subTokens)>1):\n",
        "                        for subToken in subTokens:\n",
        "                            nextTokenAbbrev += subToken[0]                        \n",
        "\n",
        "                # See if we should ignore this token because it matches the regex for tokens to ignore\n",
        "                if ( reIgnore.match(nextToken) != None ):\n",
        "                    keepToken = 0;\n",
        "\n",
        "                # Otherwise see if we should ignore this token because\n",
        "                # it is a close match to a previously selected token\n",
        "                elif len(outputTokens) > 0:          \n",
        "                    for prevToken in outputTokens:\n",
        "                        # Ignore token if it is a substring of a previous token\n",
        "                        if nextToken in prevToken:\n",
        "                            keepToken = 0\n",
        "                        # Ignore token if it is a superstring of a previous token\n",
        "                        elif prevToken in nextToken:\n",
        "                            keepToken = 0\n",
        "                        # Ignore token if it is an acronym of a previous token\n",
        "                        elif nextTokenIsAcronym:\n",
        "                            subTokens = prevToken.split('_')\n",
        "                            if (len(subTokens)>1):\n",
        "                                prevTokenAbbrev = \"\"\n",
        "                                for subToken in subTokens:\n",
        "                                    prevTokenAbbrev += subToken[0]\n",
        "                                if prevTokenAbbrev == nextToken:\n",
        "                                    keepToken = 0                                  \n",
        "                    for prevAcronym in prevAcronyms:\n",
        "                        # Ignore token if it is the long form of an earlier acronym\n",
        "                        if nextTokenAbbrev == prevAcronym:\n",
        "                                keepToken = 0\n",
        "\n",
        "                # Add tokens to the summary for this topic                \n",
        "                if keepToken:\n",
        "                    # Always add at least one token to the summary\n",
        "                    if len(topicSummary) == 0 or ( len(topicSummary) + len(nextTokenOut) + 1 < maxStringLen):\n",
        "                        if len(topicSummary) == 0:\n",
        "                            topicSummary = nextTokenOut\n",
        "                        else: \n",
        "                            topicSummary += \", \" + nextTokenOut\n",
        "                        outputTokens.append(nextToken)\n",
        "                        if nextTokenIsAcronym:\n",
        "                            prevAcronyms.append(nextToken)\n",
        "                    # If we didn't add the previous word and we're within 10 characters of \n",
        "                    # the max string length then we'll just stop here\n",
        "                    elif maxStringLen - len(topicSummary) < 10 :\n",
        "                        stop = 1\n",
        "                    # Otherwise if the current token is too long, but we still have more than\n",
        "                    # 10 characters of space left we'll just skip this one and add the next token\n",
        "                    # one if it's short enough\n",
        "                termIndex += 1         \n",
        "        topicSummaries.append(topicSummary)\n",
        "    return topicSummaries   \n",
        "    \n",
        "topicSummaries = CreateTopicSummaries(topicTermWPMI, gensim_dictionary, gensim_dictionary, 85)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rank  ID  Score  Prob  Purity  Summary\n",
            "   1   3  0.575 (20.977, 0.274) stock, term, Common, investments, short, issued, shares, net, securities, loss, debt\n",
            "   2   0  0.548 (20.540, 0.267) financial, liabilities, entities, billion, interest, consolidated, Accounting\n",
            "   3   4  0.507 (19.779, 0.257) The, activities, cash, losses, Net, gains, acquired, application, mortgage, deposits\n",
            "   4   1  0.493 (19.566, 0.252) Securities, U, securities, loans, millions, agencies, federal, states, additional\n",
            "   5   2  0.466 (19.137, 0.244) million, accounting, change, statements, Net, value, interests, available, certain\n"
          ]
        }
      ],
      "source": [
        "# Rank the topics by their prominence score in the corpus\n",
        "# The topic score combines the total weight of each a topic in the corpus \n",
        "# with a topic document purity score for topic \n",
        "# Topics with topicScore > 1 are generally very strong topics\n",
        "\n",
        "numTopics =5\n",
        "topicScore = (numTopics * topicProbs) * (2 * topicPurity)\n",
        "topicRanking = (-topicScore).argsort()\n",
        "\n",
        "print (\"Rank  ID  Score  Prob  Purity  Summary\")\n",
        "for i in range(0, numTopics):\n",
        "    topicID = topicRanking[i]\n",
        "    print (\" %3d %3d %6.3f (%5.3f, %4.3f) %s\" \n",
        "           % (i+1, topicID, topicScore[topicID], 100*topicProbs[topicID], topicPurity[topicID], topicSummaries[topicID]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
